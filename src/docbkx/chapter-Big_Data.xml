<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="chapter-Big_Data" xmlns="http://docbook.org/ns/docbook"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:svg="http://www.w3.org/2000/svg"
    xmlns:m="http://www.w3.org/1998/Math/MathML"
    xmlns:html="http://www.w3.org/1999/xhtml"
    xmlns:db="http://docbook.org/ns/docbook">

    <title>Big Data</title>
    <sect1>
        <title>What is Big Data?</title>
        <para>
            You probably heard the term Big Data -- it is one of the most hyped terms now.  But what exactly is big data?
        </para>
        <para>
            Big Data is very large, loosely structured data set that defies traditional storage
        </para>
        <mediaobject>
            <imageobject>
                <imagedata fileref="matrix.jpg" format="JPG" scale="50" />
            </imageobject>
        </mediaobject>
    </sect1>
    <sect1>
        <title>Human Generated Data and Machine Generated Data</title>
        <para>
            Human Generated Data is emails, documents, photos and tweets.  We are generating this data faster than ever.  Just imagine the number of videos uploaded to YouTube and tweets swirling around.  This data can be Big Data too.
        </para>
        <para>
            Machine Generated Data is a new breed of data.  This category consists of sensor data, and logs generated by 'machines' such as email logs, click stream logs ..etc.  Machine generated data is orders of magnitude higher than Human Generated Data.
        </para>
        <para>
            Before 'hadoop' was in the scecne, the machine generated data was mostly ignored and not captured.  It is because dealing with the volume was NOT possible, or NOT cost-effective.
        </para>
        <mediaobject>
            <imageobject>
                <imagedata fileref="bigdata_pyramid1.png" format="PNG" scale="100" />
            </imageobject>
        </mediaobject>
    </sect1>
    <sect1>
        <title>Where does Big Data come from</title>
        Original big data was the web data -- as in entire Internet!  Remember hadoop was built to index the web.  These days Big data comes from multiple sources.
        <itemizedlist>
            <listitem>
                Web Data -- still it is big data
            </listitem>
            <listitem>
                Social media data : Sites like Facebook, Twitter, Linkedin generate a large amount of data
            </listitem>
            <listitem>
                Click stream data : when users navigate a website, the clicks are logged for further analysis (like navigation patterns).  Click stream data is important in online advertising and and E-Commerce
            </listitem>
            <listitem>
                sensor data : sensors embedded in roads to monitor traffic and misc. other applications generate a large volume of data
            </listitem>
        </itemizedlist>
    </sect1>

    <sect1>
        <title>Examples of Big Data in Real world</title>
        <itemizedlist>
            <listitem>
                Facebook : has 40 PB of data and captures 100 TB / day
            </listitem>
            <listitem>
                Yahoo : 60 PB of data
            </listitem>
            <listitem>
                Twitter : 8 TB / day
            </listitem>
            <listitem>
                EBay : 40 PB of data, captures 50 TB / day
            </listitem>
        </itemizedlist>
        <mediaobject>
            <imageobject>
                <imagedata fileref="tidal_wave_of_data.png" format="PNG" scale="100" />
            </imageobject>
        </mediaobject>
    </sect1>


    <sect1>
        <title>Challenges of Big Data</title>
        <sect2>
            <title>Sheer size of BigData</title>
            <para>
                Big data is... well... big in size!  How much data constitute Big Data is not very clear cut.  So lets not get bogged down in that debate.  For a small company that is used to dealing with data in Giga bytes,  10TB of data would be BIG.  How ever for companies like Facebook and Yahoo alike,  Peta bytes (PB) is big.
            </para>
            <para>
                Just the size of big data, makes it impossible (or at least cost prohibitive) to store them in traditional storage like databases or conventional filers.
            </para>
            <para>
                We are talking about cost to store Giga byte of data.  Using traditional storage filers can cost a lot of money to store Big Data.
            </para>
        </sect2>

        <sect2>
            <title>Big Data is unstructured or semi structured</title>
            <para>
                Lot of the Big Data is unstructured.  For example clickstream log data might look like
                <code>
                    timestamp, user_id, page, referer_page
                </code> <sbr/>
                Lack of structure makes relational databases not very suited to store Big Data.
            </para>
            <para>
                Plus, not many databases can cope with storing billions of rows of data.
            </para>
        </sect2>
        <sect2>
            <title>No point in just storing big data, if we can't process it</title>
            <para>
                Storing Big Data is part of the game.  We have to process it to mine intelligence out of it.  Traditional storage systems are pretty 'dumb' as in they just store bits --  They don't offer any processing power.
            </para>
            <para>
                The traditional data processing model has data stored in 'storage cluster', data is copied over to 'compute cluster' for processing and the results are written back to storage cluster.
            </para>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="storage_compute_1.png" format="PNG" scale="100" />
                </imageobject>
            </mediaobject>
            <para>
                This model how ever doesn't quite work for Big Data.  Because copying out so much data out to compute cluster might be too time consuming or impossible.  So what is the answer?
            </para>
            <para>
                One solution is to process Big Data 'in place' -- as in storage cluster double as compute cluster.
            </para>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="storage_compute_2.png" format="PNG" scale="100" />
                </imageobject>
            </mediaobject>
        </sect2>
    </sect1>

    <sect1>
        <title>How Hadoop solves Big Data problem</title>
        <sect2>
            <title>Hadoop clusters scale horizontally</title>
            <para>
                More storage and compute power can be achieved by adding more nodes to a Hadoop cluster.  This eliminates the need to buy more and more powerful and expensive hardware.
            </para>
        </sect2>
        <sect2>
            <title>Hadoop can handle unstructured / semi-structured data</title>
            <para>
                Hadoop doesn't enforce a 'schema' on the data it stores.  It can handle arbitary text and binary data.  So Hadoop can 'digest' any unstructured data easily.
            </para>
        </sect2>

        <sect2>
            <title>Hadoop clusters provides storage and computing</title>
            <para>
                We saw how having a seperate storage and processing clusters is not the best fit for Big Data.  Hadoop clusters provide storage and distributed computing all in one.
            </para>
        </sect2>
    </sect1>
</chapter>
