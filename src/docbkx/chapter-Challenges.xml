<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="chapter-Challenges" xmlns="http://docbook.org/ns/docbook"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:svg="http://www.w3.org/2000/svg"
    xmlns:m="http://www.w3.org/1998/Math/MathML"
    xmlns:html="http://www.w3.org/1999/xhtml"
    xmlns:db="http://docbook.org/ns/docbook">

    <title>Hadoop Challenges</title>
    <para>
        This chapter explores some of the challenges in adopting Hadoop in to a company.
    </para>

    <section>
        <title>Hadoop is a cutting edge technology </title>
        <para>
            Hadoop is a new technology.  And as with adopting any new technology, finding the people who knows the technology is harder.
        </para>
    </section>

    <section>
        <title>Hadoop in the Enterprise Eco System</title>
        <para>
            Hadoop is designed to solve Big Data problems encountered by Web and Social companies.  In doing so, lot of the features Enterprises need or want are put on the back seat.  For example HDFS does not offer native support for security and authentication.
        </para>
    </section>

    <section>
        <title>Hadoop is still rough on the edges</title>
        <para>
            The development and admin tools for Hadoop are still pretty new.  Companies like Cloudera, Horton Works, MapR and Karmasphere have been working on this issue.  How ever the tooling may not be as mature as Enterprises are used to (say Oracle Admin ..etc)
        </para>
    </section>

    <section>
        <title>Hadoop is NOT cheap</title>
        <section>
            <title>Hardware Cost</title>
            <para>
                Hadoop runs on 'commodity' hardware. But these are not cheapo machines; these are server grade hardware.  For more see <xref linkend="chapter-Hardware_Software"/>
            </para>
            <para>
                So standing up a reasonably large Hadoop cluster, say 100 nodes, will cost significant amount of money.   For example, lets say a Hadoop node is $5000.  A 100 node cluster would be $500,000 for hardware.
            </para>
        </section>

        <section>
            <title>IT and Operations costs</title>
            <para>
                A large Hadoop cluster will require support from various teams like : Network Admins,  IT, Security Admins,  System Admins.
            </para>
            <para>
                Also one needs to think about operational costs like Data Center expenses : cooling, electricity ..etc
            </para>
        </section>
    </section>

    <section>
        <title>Map Reduce is a different programming paradigm</title>
        <para>
            Solving problems using Map Reduce requires a different kind of thinking.   Engineering teams would need additional training to take advantage of Hadoop.
        </para>
    </section>


    <section>
        <title>Hadoop and High Availability</title>
        <para>
            Hadoop version 1 had a Single-Point-Of-Failure problem because of Namenode.  There was only one Namenode for the cluster, and if it goes down, the whole Hadoop cluster would be in-operable.  This has prevented using Hadoop for mission-critical, always-on applications.
        </para>
        <para>
            This problem is more pronounced on paper than in reality.  Yahoo did a study that analyzed their Hadoop cluster failures.  And the study found only a tiny fraction of failures were caused by NameNode failure.<sbr/>
            TODO : link
        </para>
        <para>
            How ever this problem is being solved by various Hadoop providers.  See <xref linkend="chapter-HA"/> chapter for more details.
        </para>
    </section>


</chapter>
