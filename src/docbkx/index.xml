<?xml version="1.0" encoding="UTF-8"?>
<book xml:id="simple_book" xmlns="http://docbook.org/ns/docbook" version="5.0">
    <title>Hadoop Illuminated</title>
    <info>        
        <author>
            <personname>
                <firstname>Mark</firstname>
                <surname>Kerzner</surname>          
            </personname>
        </author>
    </info>        
    <dedication>        
        <para>To the open source community
        </para>        
    </dedication>   
    <acknowledgements>        
        <para>
            I would like to express gratitude to my editors, co-authors, colleagues, and bosses
            who shared the thorny path to working clusters - with the hope to make it less thorny for 
            those who follow. Seriously, folks, Hadoop is hard, and Big Data is tough, and there are 
            many related products and skills that you need to master. Therefore, have fun, 
            <ulink url="http://groups.google.com/group/hadoop-illuminated">provide your feedback</ulink>, 
            and I hope you will find the book entertaining. You might even find it useful.
        </para>
        <para>
            "The author's opinions do not necessarily coincide with his point of view." -             
            <ulink url="http://lib.udm.ru/lib/PELEWIN/pokolenie_engl.txt">
                <emphasis>Victor Pelevin</emphasis>
            </ulink>
        </para>
    </acknowledgements>
    <chapter xml:id="chapter_0">
        <title>Who is this book for?</title>
        <section>
            <title>About "Hadoop Illuminated"</title>
            <para>This book started out as an attempt to write for a publishing house, 
                but later it became clear that the maximum amount of fun could only be had
                by writing an open source book about open source projects, for the open community.
            </para>
            <para>
                The book is intended as a working cookbook, where why can find answers to questions
                on how to express a specific programming problem in MapReduce terms, or otherwise 
                resolve it with Hadoop and other Big Data technologies.
            </para>
            <para>
                "Hadoop Illuminated" is continuous work in progress. Techniques get added, chapters added 
                and updated. Most importantly, the code is always current, and the tests are kept in working order.
            </para>
            <section>
                <title>About the source code</title>
                <para>
                    The book's companion source code is hosted on GitHub 
                    <ulink url="https://github.com/markkerzner/HadoopIlluminated">here.</ulink>
                    Alternatively, the book can be viewed as a companion to the source code. The goal is 
                    to make the the source code project as a set of templates, where one can go to and 
                    copy a specific template, bringing it into one's own project and continuing to work 
                    by modifying it accordingly. At least I use it this way all the time, keeping the project 
                    open in my IDE for convenience.
                </para>
            </section>
        </section>
    </chapter>
    <chapter xml:id="chapter_1">
        <title>Soft introduction to Hadoop</title>
        <section>
            <title>MapReduce or Hadoop?</title>
            <para>The goal here is to present an intro to Hadoop so simple that any programmer who
                reads it will be able to write simple Hadoop solutions and run them on a Hadoop
                cluster.
            </para>
            <para>
                First, however, let us have the two basic definitions - what is Hadoop and what
                is MapReduce?
            </para>
            <para>
                <emphasis>MapReduce</emphasis> is a programming framework, 
                <ulink url="http://research.google.com/archive/mapreduce.html">published by Google in 2004</ulink>. 
                Much like other frameworks, such as Spring, Struts, or MFC, the MapReduce framework
                does some things for you, and provides the place for you to fill in the blanks. What
                MapReduce does for you is to organize your multiple computers in a cluster in
                order to perform the calculations you need. It takes care of distributing the work
                between computers and of putting together the results of each computer's
                computation. Just as important, it takes care of hardware and network failures, so
                that they do not affect the flow of your computation. You, in turn, have to break
                your problem into separate pieces which can be processed in parallel by multiple
                machines, and you provide the code to do the actual calculation.
            </para>
            <para>
                <emphasis>Hadoop</emphasis> is an open-source implementation of the MapReduce framework. For
                Google everything is MapReduce: both the way to write their applications, and the
                actual implementation are called MapReduce. For the rest of the world, the
                framework, the theory, and the way you spell out the computations are all called
                MapReduce. However, the actual software that implements this is called Hadoop.
                Let us agree for simplicity that MapReduce and Hadoop are just two names for
                the same things. Note, however, that when Google teaches college students the
                ideas of MapReduce programming, they, too, use Hadoop, because their actual        
                implementation of MapReduce is proprietary code. To emphasize the difference,
                we can note that the Hadoop engineers at Yahoo like to challenge the engineers at
                Google to sorting competitions between Hadoop and MapReduce.
            </para>
        </section>
        <section>
            <title>Why Hadoop?</title>
            <para>
                We have already mentioned that the MapReduce framework is used at Google,
                Yahoo, Facebook. It sees big uptake in finance, retail, telecom, and the
                government. It is making inroads into life sciences. Why is this?
            </para>
            <figure>
                <title>Will you join the Hadoop dance?</title>
                <graphic fileref="images/hadoop-dance-resized.png"></graphic>                
                Artwork by 
                <ulink url="https://picasaweb.google.com/110574221375288281850/RebeccaSArt">RK</ulink> 
            </figure>
            <para>
                The short answer is that it simplifies dealing with Big Data. This answer
                immediately resonates with people, it is clear and succinct, but it is not complete.
                The Hadoop framework has built-in power and flexibility to do what you could not
                do before. In fact, Cloudera presentations at the latest O'Reilly Strata conference
                mentioned that MapReduce was initially used at Google and Facebook not
                primarily for its scalability, but for what it allowed do with the data.                
            </para>
            <para>
                In 2010, the average size of the Cloudera's customers' cluster was 30 machines.
                In 2011 it was 70. When people start using Hadoop, they do it for many reasons,
                all concentrated around the new ways of dealing with the data. What gives them
                the security to go ahead is the knowledge that Hadoop solutions are massively
                scalable, as has been proved by the Hadoop running in the world's largest computer
                centers and at largest companies.                
            </para>
            <para>
                As you will discover, the Hadoop framework organizes the data and the
                computations, and then runs your code. At times, it makes sense to run your
                solution, expressed in MapReduce paradigm, even on a single machine.                
            </para>
            <para>
                But of course, Hadoop really shines when you have not one, but rather tens,
                hundreds, or thousands of computers. If your data or computations are significant
                enough (and whose aren't these days?), then you need more than one machine to do
                the number crunching. If you try to organize the work yourself, you will soon
                discover that you have to coordinate the work of many computers, handle failures,
                retries, and collect the results together, and so on. Enter Hadoop to solve all these
                problems for you. Now that you have a hammer, everything becomes a nail: people
                will often reformulate their problem in MapReduce terms, rather than create a new
                custom computation platform.                
            </para>
            <para>
                No less important than Hadoop itself are its many friends. Hadoop Distributed
                File System (HDFS) provides unlimited file space available from any Hadoop
                node. HBase is a high-performance unlimited-size database working on top of
                Hadoop. If you need the power of familiar SQL over your large datasets, Pig
                provides you with an answer. While Hadoop can be used by programmers and
                taught to students as an introduction to Big Data, its companion projects (including
                the ZooKeeper, about which we will hear later on) will make possible and
                simplify, by providing tried-and-proven frameworks, every aspect of dealing with
                large datasets.                
            </para>
            <para>
                As you learn the concepts, and perfect your skills with the techniques described
                in this book, you will discover that there are many cases where Hadoop storage,
                Hadoop computation, or Hadoop's friends can help you. Let's look at some of these
                situations.                
            </para>
            <itemizedlist>
                <listitem>
                    <para>
                        Do you find yourself often cleaning the limited hard drives in your company? Do you
                        need to transfer data from one drive to another, as a backup? Many people are so used to
                        this necessity, that they consider it an unpleasant but unavoidable part of life. Hadoop
                        distrubuted file system, HDFS, grows by adding servers. To you it looks like one hard
                        drive. It is self-replicating (you set the replication factor) and thus provides redundancy
                        as a software alternative to RAID.
                    </para>
                </listitem>
                <listitem>
                    <para>
                        Do your computations take an unacceptably long time? Are you forced to give up on
                        projects because you don’t know how to easily distribute the computations between
                        multiple computers? MapReduce helps you solve these problems. What if you don’t have
                        the hardware to run the cluster? - Amazon EC2 can run MapReduce jobs for you, and you
                        pay only for the time that it runs - the cluster is automatically formed for you and then
                        disbanded.                        
                    </para>
                </listitem>
                <listitem>
                    <para>
                        But say you are lucky, and instead of maintaining legacy software, you are charged with
                        building new, progressive software for your company's workflow. Of course, you want to
                        have unlimited storage, solving this problem once and for all, so as to concentrate on
                        what's really important. The answer is: you can mount HDFS as FUSE file system, and
                        you have your unlimited storage. In our cases studies we look at the successful use of the
                        HDFS as as grid storage for the Large Hadron Collider.                        
                    </para>
                </listitem>
                <listitem>
                    <para>
                        Imagine you have multiple clients using your online resources, computations, or data.
                        Each single use is saved in a log, and you need to generate a summary of use of resources
                        for each client by day or by hour. From this you will do your invoices, so it IS important.
                        But the data set is large. You can write a quick MapReduce job for that. Better yet, you
                        can use Hive, a data warehouse infrastructure built on top of Hadoop, with its ETL
                        capabilities, to generate your invoices in no time. We'll talk about Hive later, but we hope
                        that you already see that you can use Hadoop and friends for fun and profit.                        
                    </para>
                </listitem>
            </itemizedlist>
            <para>
                Once you start thinking without the usual limitations, you can improve on what
                you already do and come up with new and useful projects. In fact, this book
                partially came about by asking people how they used Hadoop in their work. You,
                the reader, are invited to submit your applications that became possible with
                Hadoop, and I will put it into Case Studies, with attribution :). Of course.
            </para>
        </section>
        <section>
            <title>Meet the Hadoop Zoo</title>
            <para>
                QUINCE
            </para>
            <para>
                Is all our company here?
            </para>
            <para>                
                BOTTOM
            </para>
            <para>
                You were best to call them generally, man by man, according to the scrip.
            </para>
            <para>
                <ulink url="http://shakespeare.mit.edu/midsummer/full.html">
                    Shakespeare, "Midsummer Night's Dream"
                </ulink>
            </para>
            <para>
                There are a number of animals in the Hadoop zoo, and each deals with a certain
                aspect of Big Data. Let us illustrate this with a picture, and then introduce them
                one by one.                
            </para>
            <figure>
                <title>The Hadoop Zoo</title>
                <graphic fileref="images/hadoop-zoo-resized.png"></graphic>                
                Artwork by 
                <ulink url="https://picasaweb.google.com/110574221375288281850/RebeccaSArt">RK</ulink> 
            </figure>        
            <section>
                <title>HDFS - Hadoop Distributed File System</title>
            
                <para>
                    HDFS, or Hadoop Distributed File System, gives the programmer unlimited
                    storage. Unlimited storage is a programmers's dream, so you can skip the rest.
                    However, here are the additional advantages of HDFS.                
                </para>
                <itemizedlist>
                    <listitem>
                        <para>
                            Horizontal scalability. Thousands of servers holding petabytes of data. When you need
                            more more storage, you don't switch to more expensive solutions, but add servers to it.                        
                        </para>
                    </listitem>
                    <listitem>
                        <para>
                            Commodity hardware. The HDFS is designed with relatively cheap commodity hardware
                            in mind. HDFS is self-healing and replicating.                      
                        </para>
                    </listitem>
                    <listitem>
                        <para>
                            Fault tolerance. Every member of the Hadoop zoo knows how to deal with hardware
                            failures. If you have 10 thousand servers, then you will see one server fail every day, on
                            the average. HDFS foresees that by replicating the data, by default three times, on
                            different data node servers. Thus, if one data node fails, the other two can be used to
                            restore the third one in a different place.
                        </para>
                    </listitem>                                    
                </itemizedlist>
                <para>
                    The HDFS implementation is modeled after GFS, Google Distributed File
                    system, thus you can read the first paper on this, to be found here:
                    http://labs.google.com/papers/gfs.html.                
                </para>
            </section>
            <section>
                <title>Hadoop, the little elephant</title>            
                <para>
                    Hadoop organizes your computations. It reads the data, usually from HDFS, in
                    blocks. However, it can read the data from other places too, including mounted
                    local file systems, web, and databases. It divides the computations between
                    different computers (servers, or nodes). It is also fault-tolerant.
                </para>
                <para>
                    If some of your nodes fail, Hadoop knows how to continue with the
                    computation, by re-assigning the incomplete work to another node and cleaning up
                    after that node that could not complete its task. It also knows how to combine the
                    results of the computation in one place.                
                </para>
            </section>
            <section>
                <title>HBase, the database for Big Data</title>
                <para>"Thirty spokes share the wheel's hub, it is the empty space that make it useful" 
                    - Tao Te Ching (
                    <ulink url = "http://terebess.hu/english/tao/gia.html">
                        translated by Gia-Fu Feng and Jane English)
                    </ulink>
                </para>
                <para>
                    Not properly an animal, HBase is nevertheless very powerful. It is currently
                    denoted by the letter H with a base clef. If you think this is not so great, you are
                    right, and the HBase people are thinking of changing the logo. HBase is a database
                    for Big Data, up to millions of columns and billions of rows.
                </para>
                <para>
                    Another feature of HBase is that it is a key-value database, not a relational
                    database. We will get into the pros and cons of these two approaches to databases
                    later, but for now let's just note that key-value databases are considered as more
                    fitting for Big Data. Why? Because they don't store nulls! This gives them the appellation 
                    of "sparse," and as we saw above, Tao Te Chin says that they are useful for this reason.
                </para>
            </section>
            <section>
                <title>ZooKeeper</title>
                <para>
                    Every zoo has a zoo keeper, and the Hadoop zoo is no exception. When all the
                    Hadoop animals want to do something together, it is the ZooKeeper who helps
                    them do it. They all know him and listen and obey his commands. Thus, the
                    ZooKeeper is a centralized service for maintaining configuration information,
                    naming, providing distributed synchronization, and providing group services.
                </para>
                <para>  
                    ZooKeeper is also fault-tolerant. In your environment development, you can put
                    the zookeeper on one node, but in production you usually run it on an odd number
                    of servers, such as 3 or 5.
                </para>
            </section>
            <section>
                <title>Hive - data warehousing</title>
                <para>
                    Hive: "I am Hive, I let you in and out of the HDFS cages, and you can talk SQL to me!"
                </para>                
                <para>
                    Hive is a way for you to get all the honey, and to leave all the work to the bees.
                    You can do a lot of data analysis with Hadoop, but you will also have to write
                    MapReduce tasks. Hive tasks it upon itself. Hive defines a simple SQL-like query
                    language, called QL, that enables users familiar with SQL to query the data.
                </para>
                <para>
                    At the same time, if your Hive program does almost what you need, but not
                    quite, you can call on your MapReduce skill. Hive allows you to write custom
                    mappers and reducers to extend the QL capabilities.
                </para>
            </section>
            <section>
                <title>
                    Pig - Big Data manipulation
                </title>
                <para>
                    Pig: "I am Pig, I let you move HDFS cages around, and I speak Pig Latin."
                </para>
                <para>
                    Pig is called this way not because it eats a lot, although you can imagine a pig
                    pushing around and consuming big volumes of information. Rather, it is called this
                    way because it speaks Pig Latin. Others who also speak this language are the kids -
                    or the programmers, who visit the Hadoop zoo.
                </para>
                <para>
                    So what is Pig Latin that Apache Pig speaks? As a rough analogy, if Hive is the
                    SQL of Big Data, then Pig Latin is the language of the stored procedures of Big
                    Data. It allows you to manipulate large volume of information, analyze them, and
                    create new derivative data sets. Internally it creates a sequence of MapReduce jobs,
                    and thus you, the programmer-kid, can use this simple language to solve pretty
                    sophisticated large-scale problems.
                </para>
            </section>
        </section>
        <section>
            <title>Hadoop alternatives</title>
            <para>
                Now that we have met the Hadoop zoo, we are ready to start our excursion. Only
                one thing stops are at this point - and that is, a gnawing doubt, are we in the right
                zoo? Let us look at some alternatives to dealing with Big Data. Granted, our
                concentration here is Hadoop, and we may not give justice to all the other
                approaches. But we will try.
            </para>
            <section>
                <title>Large data storage alternatives</title>
                <para>
                    HDFS is not the only, and in fact, not the earliest or the latest distributed file
                    system. CEPH claims to be more flexible and to remove the limit on the number of
                    files. HDFS stored all of its file information in the memory of the server which is
                    called the NameNode. This is its strong point - speed - but it is also its Achilles'
                    heel. CEPH, on the other hand, makes the function of the NameNode completely
                    distributed.
                </para>
                <para>
                    Another possible contender is ZFS, an open-source file system from SUN, and
                    currently Oracle. Intended as a complete redesign of file system thinking, ZFS
                    holds a strong promise of unlimited size, robustness, encryption, and many other
                    desirable qualities built into the low-level file system. After all, Hadoop and its
                    model GFS both build on a conventional file system, creating their improvement
                    on top of it, and the premise of ZFS is that the underlying file system should be
                    redesigned to address the core issues.
                </para>
                <para>
                    I have seen production architectures built on ZFS, where the data storage
                    requirements were very clear and well-defined and where storing data from
                    multiple field sensors was considered better done with ZFS. The pros for ZFS in
                    this case were: built-in replication, low overhead, and - given the right structured
                    of records when written - build-in indexing for searching. Obviously, this was a
                    very specific, though very fitting solution.
                </para>
                <para>
                    While other file system start out with the goal of improving on HDFS/GFS
                    design, the advantage of HDFS is that it is very widely used. I think that in
                    evaluating other file systems, the reader can be guided by the same considerations
                    that led to the design of GFS: the designer of it analyzed prevalent file usage in the
                    majority of their applications, and created a file system that optimized reliability
                    for that particular type of usage. The reader may be well advised to compare the
                    assumption of GFS designers with his or her case, and decide if HDFS fits the
                    purpose, or if something else should be used in its place.
                </para>
                <para>
                    We should also note here that we compare Hadoop to other open-source storage
                    solutions. There are proprietary and commercial solutions, but such comparison
                    goes beyond the scope of this introduction.
                </para>
            </section>
            <section>
                <title>Large database alternatives</title>
                <para>
                    The closest to HBase is Cassandra. While HBase is a near-clone of Google’s
                    BigTable, Cassandra purports to being a “BigTable/Dynamo hybrid”. It can be said
                    that while Cassandra’s “writes-never-fail” emphasis has its advantages, HBase is
                    the more robust database for a majority of use-cases. HBase being more prevalent
                    in use, Cassandra faces an uphill battle - but it may be just what you need.
                </para>
                <para>
                    Hypertable is another database close to Google's BigTable in features, and it
                    claims to run 10 times faster than HBase. There is an ongoing discussion between
                    HBase and Hypertable proponents, and the authors do not want to take sides in it,
                    leaving the comparison to the reader. Like Cassandra, Hypertable has less users
                    than HBase, and here too, the reader needs to evaluate the speed of Hypertable for
                    his application, and weight it with other factors.
                </para>
                <para>
                    MongoDB (from "humongous") is a scalable, high-performance, open source,
                    document-oriented database. Written in C++, MongoDB features
                    document-oriented storage, full index on any attribute, replication and high
                    availability, rich, document-based queries, and it works with MapReduce. If you
                    are specifically processing documents and not arbitrary data, it is worth a look.
                </para>
                <para>
                    Other open-source and commercial databases that may be given consideration
                    include Vertica with its SQL support and visualization, Cloudran for OLTP, and
                    Spire.
                </para>
                <para>
                    In the end, before embarking on a development project, you will need to
                    compare alternatives. Below is an example of such comparison. Please keep in
                    mind that this is just possible point of view, and that the specifics of your project
                    and of your view will be different. Therefore, the table below is mainly to
                    encourage the reader to do a similar evaluation for his own needs.
                </para>
            </section>            
            <table frame='all'>
                <title>Comparison of Big Data </title>
                <tgroup cols='6' align='left' colsep='1' rowsep='1'>
                    <colspec colname='c1'/>
                    <colspec colname='c2'/>
                    <colspec colname='c3'/>
                    <colspec colnum='5' colname='c5'/>
                    <thead>
                        <row>
                            <entry>DB Pros/Cons</entry>
                            <entry>HBase</entry>
                            <entry>Cassandra</entry>
                            <entry>Vertica</entry>
                            <entry>CloudTran</entry>
                            <entry>HyperTable</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>Pros</entry>
                            <entry>Key-based NoSQL, active user community, Cloudera support </entry>
                            <entry>Key-based NoSQL, active user community, Amazon's Dynamo on EC2</entry>
                            <entry>Closed-source, SQL-standard, easy to use, visualization tools, complex queries </entry>
                            <entry>Closed-source optimized online transaction processing </entry>
                            <entry>Drop-in replacement for HBase, open-source, arguably much faster</entry>
                        </row>
                        <row>
                            <entry>Cons</entry>
                            <entry>Steeper learning curve, less tools, simpler queries</entry>
                            <entry>Steeper learning curve, less tools, simpler queries</entry>
                            <entry>Vendor lock-in, price, RDMS/BI - may not fit every application </entry>
                            <entry>Vendor lock-in, price, transaction-optimized, may not fit every application, needs wider adoption</entry>
                            <entry>New, needs user adoption and more testing</entry>
                        </row>
                        <row>
                            <entry>Notes</entry>
                            <entry>Good for new, long-term development </entry>
                            <entry>Easy to set up, no dependence on HDFS, fully distributed architecture</entry>
                            <entry>Good for existing SQL-based applications that needs fast scaling </entry>
                            <entry>Arguably the best OLTP </entry>
                            <entry>To be kept in mind as a possible alternative </entry>
                        </row>                        
                    </tbody>
                </tgroup>                
            </table>            
        </section>
        <section>
            <title>Alternatives for distributed massive computations</title>
            <para>Here too, depending upon the type of application that the reader needs, other
                approaches make prove more useful or more fitting to the purpose.
            </para>
            <para>
                The first such example is the JavaSpaces paradigm. JavaSpaces is a giant hash
                map container. It provides the framework for building large-scale systems with
                multiple cooperating computational nodes. The framework is thread-safe and
                fault-tolerant. Many computer working on the same problem can store their data in
                a JavaSpaces container. When a node wants to do some work, it finds the data in
                the container, check its out, works on it, and then returns it back. The framework
                provides for atomicity. While the node is working on the data, other nodes cannot
                see it. If it fails, its lease on the data expires, and the data is returned back to the
                pool of data for processing.
            </para>
            <para>
                The champion of JavaSpaces is a commercial company called GigaSpaces. The
                license for a JavaSpaces container from GigaSpaces is free - provided that you can
                fit into the memory of one computer. Beyond that, GigaSpaces has implemented
                unlimited JavaSpaces container where multiple servers combine their memories
                into a shared pool. GigaSpaces has created a big sets of additional functionality for
                building large distributed systems. So again, everything depends on the reader's
                particular situation.                
            </para>
            <para>
                GridGain is another Hadoop alternative. The proponents of GridGain claim that
                while Hadoop is a compute grid and a data grid, GridGain is just a compute grid,
                so if your data requirements are not huge, why bother? They also say that seems to
                be enormously simpler to use. Study of the tools and prototyping with them can
                give one a good feel for the most fitting answer.                
            </para>
            <para>
                Terracotta is a commercial open source company, and in the open source realm
                it provides Java big cache and a number of other components for building large
                distributed systems. One of its advantages is that it allows to scale existing
                applications without a significant rewrite. By now we have gotten pretty far away
                from Hadoop, which proves that we have achieved our goal - give a reader a quick
                overview of various alternatives for building large distributed systems. Success in
                whichever way you choose to go!                
            </para>            
        </section>
        <section>
            <title>Arguments for Hadoop</title>
            <para>
                We have given the pro arguments for the Hadoop alternatives, but now we can put
                in a word for the little elephant and its zoo. It boasts wide adoption, has an active
                community, and has been in production use in many large companies. I think that
                before embarking on an exciting journey of building large distributed systems, the
                reader will do well to view the presentation by Jeff Dean, a Google Fellow, on the
                "Design, Lessons, and Advice from Building Large Distributed Systems"                                 
                <ulink url = "http://www.slideshare.net/xlight/google-designs-lessons-and-advice-from-building-large-distributed-systems">
                    found on SlideShare
                </ulink>
            </para>
            <para>
                Google has built multiple applications on GFS, MapReduce, and BigTable, which
                are all implemented as open-source projects in the Hadoop zoo. According to Jeff,
                the plan is continue with 1,000,000 to 10,000,000 machines spread at 100s to
                1000s locations around the world, and as arguments go, that is pretty big.
            </para>
        </section>
        <section>
            <title>Say "Hi!" to Hadoop</title>
            <para>Enough words, let’s look at some code! 
                First, however, let us explain how MapReduce works in human terms.
            </para>
            <section>
                <title>A dialog between you and Hadoop</title>
                <para>
                    Imagine you want to count word frequencies in a text. It may be a book or a
                    document, and word frequencies may tell you something about its subject. Or it
                    may be a web access log, and you may be looking for suspicious activity. It may be
                    a log of any customer activity, and you might be looking for most frequent
                    customers, and so on.                    
                </para>
                <para>
                    In a straightforward approach, you would create an array or a hash table of
                    words, and start counting the word's occurrences. You may run out of memory, or
                    the process may be slow. If you try to use multiple computers all accessing shared
                    memory, the system immediately gets complicated, with multi-threading, and we
                    have not even thought of hardware failures. Anyone who has gone through similar
                    exercises know how quickly a simple task can become a nightmare.                    
                </para>
                <para>
                    Enter Hadoop and offers its services. The following dialog ensues.
                </para>
                <para>
                    <emphasis>Hadoop: </emphasis>How can I help?
                </para>   
                <para>
                    <emphasis>You: </emphasis>I need to count words in a document.
                </para>
            </section>

        </section>
    </chapter>
    <chapter xml:id="chapter_2">
        <title>Get your hands dirty with code</title>
        <para>
            Programmers like playing around with the code. In the long-standing philosophical 
            discussion of what is more important, theory or practice, it was concluded that 
            theory (reading this book) is more important, because it leads to practice (coding). 
        </para>
        <section>
            <title>Hands-on: setting your development environment</title>  
            <section>
                <title>Setting your IDE</title>
            </section>
            <section>
                <title>Setting Hadoop in local mode</title>
            </section>
            <section>
                <title>Setting a Hadoop cluster on EC2</title>
            </section>
            <section>
                <title>Using S3 instead of HDFS on EC2 Hadoop Clusters</title>
            </section>
            <section>
                <title>Setting HBase on EC2</title>
            </section>
        </section>
    </chapter>
    <chapter xml:id="chapter_3">
        <title>Questions of Sorting</title>
        <para>Let's sort it out</para>
    </chapter>
</book>