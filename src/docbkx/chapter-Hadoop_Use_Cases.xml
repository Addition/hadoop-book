<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="chapter-Hadoop_Use_Cases" xmlns="http://docbook.org/ns/docbook"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:svg="http://www.w3.org/2000/svg"
    xmlns:m="http://www.w3.org/1998/Math/MathML"
    xmlns:html="http://www.w3.org/1999/xhtml"
    xmlns:db="http://docbook.org/ns/docbook">

    <title>Hadoop Use Cases and Case Studies</title>
    <para>
        These are collection of some use cases of Hadoop.  This is not meant to be an exhaustive list, but a sample to give you some ideas.
    </para>
    <para>
        A pretty extensive list is available at <link xlink:href="http://wiki.apache.org/hadoop/PoweredBy">Powered By Hadoop site</link>
    </para>
    <!-- template
        <section>
            <title>Dodd-Frank Compliance @  a bank</title>
            <para>
            intro
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                problem
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                solution
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                Cloudera
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                20+ nodes;  1TB of data / month
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/connect_case_study_datameer_banking_financial.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/connect_case_study_datameer_banking_financial.pdf">cached version</link>)
                (Published Nov 2012)
                <sbr/>
            </para>
        </section>
        -->

    <section>
        <title>Politics</title>
        <section>
            <title>2012 US Presidential Election</title>
            <para>
                <link xlink:href="http://www.computerworld.com/s/article/9233587/Barack_Obama_39_s_Big_Data_won_the_US_election#disqus_thread">How Big Data help Obama win re-election</link>  - by Michael Lynch, the founder of <link xlink:href="http://www.autonomy.com/">Autonomy</link>
            </para>
        </section>
    </section>

    <section>
        <title>Data Storage</title>
        <section>
            <title>Net App</title>
            <para>
                Net App collects diagnostic data from its storage systems deployed at customer sites.  This data is used to analyze the health of Net App systems.

            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                Net App collects over 600,000 data transactions weekly, consisting of unstructured logs and system diagnostic information.  Tradional data storage system proved in-adequate to capture and process this data.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                Cloudera Hadoop system captures the data and allows parallel processing of data.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                Cloudera
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                30+ nodes;  7TB of data / month
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/Cloudera_Case_Study_NetApp.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/Cloudera_Case_Study_NetApp.pdf">cached version</link>)
                (Published Sep 2012)
                <sbr/>
            </para>
        </section>
    </section>

    <section>
        <title>Financial Services</title>
        <section>
            <title>Dodd-Frank Compliance @  a bank</title>
            <para>
                A leading retail bank is using Cloudera and Datameer to validate data accuracy and quality to comply with regulations like Dodd-Frank

            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
Previous solution using Teradata and IBM Netezza was time consuming and complex, and the data mart approach didn’t provide the data completeness required for determining overall data quality.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                Cloudera + Datameer platform allows analysing trillions of records which currently result in approximately one terabyte per month of reports. The results are reported through a data quality dashboard.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                Cloudera + Datameer
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                20+ nodes;  1TB of data / month
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/connect_case_study_datameer_banking_financial.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/connect_case_study_datameer_banking_financial.pdf">cached version</link>)
                (Published Nov 2012)
                <sbr/>
            </para>
        </section>
    </section>


    <section>
        <title>Health Care</title>
        <section>
            <title>Storing and processing Medical Records</title>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                This health IT company instituted a policy of saving seven years’ historical claims and remit data, but its in-house database systems had trouble meeting the data retention requirement while processing millions of claims every day
            </para>
            <para>
                <emphasis role="bold">Solution:</emphasis><sbr/>
                Hadoop system allows archiving seven years’ claims and remit data, which requires complex processing to get into a normalized format. Logging terabytes of data generated from transactional systems daily, and storing them in CDH for analytical purposes
            </para>
            <para>
                <emphasis role="bold">Hadoop vendor :</emphasis><sbr/>
                Cloudera
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                10+ nodes pilot;  1TB of data / day
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/Cloudera_Case_Study_Healthcare.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/Cloudera_Case_Study_Healthcare.pdf">cached version</link>)
                (Published Oct 2012)
                <sbr/>
            </para>
        </section>
    </section>


    <section>
        <title>Human Sciences</title>

        <section>
            <title>NextBio</title>
            <para>
                NextBio is using Hadoop MapReduce and HBase to process massive amounts of human gnome data.
            </para>
            <para>
                <emphasis role="bold">Problem:</emphasis><sbr/>
                processing multi-tera byte data sets wasn't feasible using tradional databases like mysql.
            </para>
            <para>
                <emphasis role="bold">Solution:</emphasis><sbr/>
                NextBio uses Hadoop map reduce to process gnome data in batches.  And it uses HBase as a scalable data store
            </para>
            <para>
                <emphasis role="bold">Hadoop vendor :</emphasis><sbr/>
                Intel
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.nextbio.com/">NextBio</link><sbr/>
                <link xlink:href="http://hadoop.intel.com/pdfs/IntelNextBioCaseStudy.pdf">Intel case study</link> (<link xlink:href="cached_reports/IntelNextBioCaseStudy.pdf">cached version</link>)
                (Published Feb 2013)
                <sbr/>
                <link xlink:href="http://www.informationweek.com/software/information-management/hbase-hadoops-next-big-data-chapter/232901601?pgno=1">Information Week article (May 2012)</link>
            </para>

        </section>

    </section> <!-- end human sciences -->


    <section>
        <title>Telecom</title>
        <section>
            <title>China Telecom Guangdong</title>
            <para>
                <emphasis role="bold">Problem:</emphasis> Storing billions of mobile call records. And providing real time access to the call records and billing information to customers.  <sbr/>
                Tradional storage/database systems couldn't scale to the loads and provide a cost effective solution
            </para>
            <para>
                <emphasis role="bold">Solution:</emphasis> Hbase is used to store billions of rows of call record details.  30TB of data is added monthly
            </para>
            <para>
                <emphasis role="bold">Hadoop vendor :</emphasis> Intel
            </para>
            <para>
                <emphasis role="bold">Hadoop cluster size :</emphasis> 100+ nodes
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://gd.10086.cn/">China Telecom Quangdong</link><sbr/>
                <link xlink:href="http://hadoop.intel.com/pdfs/IntelChinaMobileCaseStudy.pdf">Intel case study</link> (<link xlink:href="cached_reports/IntelChinaMobileCaseStudy.pdf">cached version</link>)
                (Published Feb 2013)
                <sbr/>
                <link xlink:href="http://www.slideshare.net/IntelAPAC/apac-big-data-dc-strategy-update-for-idh-launch-rk">Intel APAC presentation</link>
            </para>

        </section>

        <section>
            <title>Nokia</title>
            <para>
                Nokia collects and analyzes vast amounts of data from mobile phones
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                <sbr/>
                (1) Dealing with 100TB structured data and 500TB+ of semi-structured data <sbr/>
                (2) 10s of PB across Nokia, 1TB / day
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                HDFS data ware house allows storing all the semi/multi structured data and offers processing data at Peta byte scale
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                Cloudera
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                (1) 500TB of data
                (2) 10s of PB across Nokia, 1TB / day
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                (1) <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/Cloudera_Nokia_Case_Study_Hadoop.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/Cloudera_Nokia_Case_Study_Hadoop.pdf">cached version</link>)
                (Published Apr 2012)
                <sbr/>
                <sbr/>
                (2) <link xlink:href="http://cdn.oreillystatic.com/en/assets/1/event/85/Big%20Data%20Analytics%20Platform%20at%20Nokia%20%E2%80%93%20Selecting%20the%20Right%20Tool%20for%20the%20Right%20Workload%20Presentation.pdf">strata NY 2012 presentation slides</link>
                (<link xlink:href="cached_reports/Nokia_Bigdata.pdf">cached version</link>)
                <sbr/>
                <link xlink:href="http://strataconf.com/stratany2012/public/schedule/detail/26880">Strata NY 2012 presentation</link>
            </para>
        </section>
    </section>

    <section>
        <title>Travel</title>
        <section>
            <title>Orbitz</title>
            <para>
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                Orbitz generates tremendous amount of log data.  The raw logs are only stored for a few days because of costly data warehouse.  Orbitz needed a effective way to store and process this data;  Plus they needed to improve their hotel rankings
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                Hadoop cluster provided very cost effective way to store vast amounts of raw logs.  Data is cleaned and analyzed.  Machine learning algorithms are run
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.slideshare.net/jseidman/windy-citydb-final-4635799">Orbitz presentation</link>
                (Published 2010)
                <sbr/>
                <link xlink:href="http://www.datanami.com/datanami/2012-04-26/six_super-scale_hadoop_deployments.html">Datanami article</link>
            </para>
        </section>
    </section>

    <section>
        <title>Energy</title>
        <section>
            <title>Siesmic Data @ Chevron</title>
            <para>
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                Chevron analyzes vast amount of seismic data to find potential oil reserves.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                Hadoop offers the storage capacity and processing power to analyze this data
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                IBM Big Insights
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://almaden.ibm.com/colloquium/resources/Managing%20More%20Bits%20Than%20Barrels%20Breuning.PDF">Presentation</link>
                (<link xlink:href="cached_reports/IBM_Chevron.pdf">cached version</link>)
                (Published June 2012)
                <sbr/>
            </para>
            <para>
            </para>
        </section>
        <section>
            <title>OPower</title>
            <para>
            Opower works with utility companies to provide engaging, relevant, and personalized content about home energy use to millions of households
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                Collecting and analyzing massive amounts of data and deriving insights into customers' energy usage.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                Hadoop provides a single storage for all the massive data.  And machine learning algorithms are run on the data.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://cdn.oreillystatic.com/en/assets/1/event/85/Data%20Science%20with%20Hadoop%20at%20Opower%20Presentation.pdf">presentation</link>
                (<link xlink:href="cached_reports/Opower.pdf">cached version</link>)
                (Published Oct 2012)
                <sbr/>
                <link xlink:href="http://strataconf.com/stratany2012/public/schedule/detail/25736">Strata NY 2012</link>
                <sbr/>
                <link xlink:href="http://www.opower.com">Opower.com</link>
            </para>
        </section>
    </section>

    <!--
    <section>
        <title>older case studies</title>

    <glosslist>
        <glossentry>
            <glossterm>Indexing the Web</glossterm>
            <glossdef>
                <para>
                    Original use case for Hadoop at Yahoo, and still is.
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>Social Media</glossterm>
            <glossdef>
                <para>
                    Hadoop is used to handle massive amounts of data generated by social media.
                </para>
                <para>
                    <itemizedlist>
                        <listitem>
                            Facebook
                            <itemizedlist>
                                <listitem>
                                    <link xlink:href="http://www.facebook.com/note.php?note_id=454991608919#">Facebook Messages built on Hadoop HBase</link>
                                </listitem>
                                <listitem>
                                    <link xlink:href="http://www.slideshare.net/zshao/hive-data-warehousing-analytics-on-hadoop-presentation?from=ss_embed">Analytics using Hive</link>
                                </listitem>
                            </itemizedlist>
                        </listitem>

                        <listitem>
                            Twitter :
                            <link xlink:href="http://www.slideshare.net/kevinweil/hadoop-at-twitter-hadoop-summit-2010">Hadoop at Twitter</link>
                        </listitem>

                        <listitem>
                            LinkedIn :
                            <link xlink:href="http://engineering.linkedin.com/hadoop">Hadoop at LinkedIn</link>
                        </listitem>
                        <listitem>
                            Measuring social influence : <link xlink:href="http://corp.klout.com/blog/category/engineering/">Klout</link>
                        </listitem>
                    </itemizedlist>
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>Serving advertisements and optimizing advertising</glossterm>
            <glossdef>
                <para>
                    Hadoop is used for storing ad logs and click logs, analyze it and tweak ad campaigns
                </para>
                <para>
                    Companies : too many to list
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>IT Security</glossterm>
            <glossdef>
                <para>
                    Network logs, intrusion detection, SPAM detection
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>Infrastructure</glossterm>
            <glossdef>
                <para>
                    appliance's 'phone home' data
                </para>
                <para>
                    Companies : NetApp
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>Finance Fraud Detection</glossterm>
            <glossdef>
                <para>
                </para>
                <para>
                    Companies : PayPal
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>Bio Sciences</glossterm>
            <glossdef>
                <para>
                    Gnome decoding
                </para>
                <para>
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>Mobile Data</glossterm>
            <glossdef>
                <para>
                    All geo data sent by mobile phones (e.g while using GPS or Maps apps)
                </para>
                <para>
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>E-Commerce</glossterm>
            <glossdef>
                <para>
                    Buying patterns and recommendations
                </para>
                <para>
                    Companies : EBay, Amazon
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>Image Processing</glossterm>
            <glossdef>
                <para>
                </para>
                <para>
                    Companies : Skybox (process Hi-Def satellite images)
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>Sensor Data</glossterm>
            <glossdef>
                <para>
                    Traffic Sensors, weather sensors
                </para>
                <para>
                </para>
            </glossdef>
        </glossentry>

        <glossentry>
            <glossterm>Machine Learning</glossterm>
            <glossdef>
                <para>
                </para>
                <para>
                </para>
            </glossdef>
        </glossentry>
    </glosslist>
    </section>
    -->

</chapter>
