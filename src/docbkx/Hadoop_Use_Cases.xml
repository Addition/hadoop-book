<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="Hadoop_Use_Cases" xmlns="http://docbook.org/ns/docbook"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:svg="http://www.w3.org/2000/svg"
    xmlns:m="http://www.w3.org/1998/Math/MathML"
    xmlns:html="http://www.w3.org/1999/xhtml"
    xmlns:db="http://docbook.org/ns/docbook">

    <title>Hadoop Use Cases and Case Studies</title>
    <para>
        This is a collection of some use cases of Hadoop.  This is not meant to be an exhaustive list, but a sample to give you some ideas.
    </para>
    <para>
        A pretty extensive list is available at the <link xlink:href="http://wiki.apache.org/hadoop/PoweredBy">Powered By Hadoop site</link>
    </para>
    <!-- template
        <section>
            <title>Dodd-Frank Compliance @  a bank</title>
            <para>
            intro
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                problem
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                solution
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                Cloudera
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                20+ nodes;  1TB of data / month
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/connect_case_study_datameer_banking_financial.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/connect_case_study_datameer_banking_financial.pdf">cached copy</link>)
                (Published Nov 2012)
                <sbr/>
            </para>
        </section>
        -->

    <section>
        <title>Politics</title>
        <section>
            <title>2012 US Presidential Election</title>
            <para>
                <link xlink:href="http://www.computerworld.com/s/article/9233587/Barack_Obama_39_s_Big_Data_won_the_US_election">How Big Data help Obama win re-election</link>  - by Michael Lynch, the founder of <link xlink:href="http://www.autonomy.com/">Autonomy</link>  
                (<link xlink:href="cached_reports/Barack_Obama_Big_Data_won_the_US_election__Computerworld.pdf">cached copy</link>)
            </para>
        </section>
    </section>

    <section>
        <title>Data Storage</title>
        <section>
            <title>NetApp</title>
            <para>
                NetApp collects diagnostic data from its storage systems deployed at customer sites.  This data is used to analyze the health of NetApp systems.

            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                NetApp collects over 600,000 data transactions weekly, consisting of unstructured logs and system diagnostic information.  Traditional data storage systems proved inadequate to capture and process this data.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                A Cloudera Hadoop system captures the data and allows parallel processing of data.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                Cloudera
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                30+ nodes;  7TB of data / month
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/Cloudera_Case_Study_NetApp.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/Cloudera_Case_Study_NetApp.pdf">cached copy</link>)
                (Published Sep 2012)
                <sbr/>
            </para>
        </section>
    </section>

    <section>
        <title>Financial Services</title>
        <section>
            <title>Dodd-Frank Compliance at a bank</title>
            <para>
                A leading retail bank is using Cloudera and Datameer to validate data accuracy and quality to comply with regulations like Dodd-Frank

            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
The previous solution using Teradata and IBM Netezza was time consuming and complex, and the data mart approach didn’t provide the data completeness required for determining overall data quality.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                A Cloudera + Datameer platform allows analyzing trillions of records which currently result in approximately one terabyte per month of reports. The results are reported through a data quality dashboard.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                Cloudera + Datameer
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                20+ nodes;  1TB of data / month
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/connect_case_study_datameer_banking_financial.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/connect_case_study_datameer_banking_financial.pdf">cached copy</link>)
                (Published Nov 2012)
                <sbr/>
            </para>
        </section>
    </section>


    <section>
        <title>Health Care</title>
        <section>
            <title>Storing and processing Medical Records</title>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                A health IT company instituted a policy of saving seven years of historical claims and remit data, but its in-house database systems had trouble meeting the data retention requirement while processing millions of claims every day
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis><sbr/>
                A Hadoop system allows archiving seven years’ claims and remit data, which requires complex processing to get into a normalized format, logging terabytes of data generated from transactional systems daily, and storing them in CDH for analytical purposes
            </para>
            <para>
                <emphasis role="bold">Hadoop vendor: </emphasis><sbr/>
                Cloudera
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                10+ nodes pilot;  1TB of data / day
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/Cloudera_Case_Study_Healthcare.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/Cloudera_Case_Study_Healthcare.pdf">cached copy</link>)
                (Published Oct 2012)
                <sbr/>
            </para>
        </section>

        <section>
            <title>Monitoring patient vitals at Los Angeles Children's Hospital</title>
            <para>
            Researchers at LA Children's Hospital is using Hadoop to capture and analyze medical sensor data.
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                Collecting lots (billions) of data points from sensors / machines attached to the patients.  This data was periodically purged before because storing this large volume of data on expensive storage was  cost-prohibitive.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                Continuously streaming data from sensors/machines is collected and stored in HDFS.  HDFS provides scalable data storage at reasonable cost.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                Unknown
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                ???
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.youtube.com/watch?v=NmMbFM7l1Rs">video</link>
                <sbr/>
                <link xlink:href="http://siliconangle.com/blog/2013/06/27/leveraging-hadoop-to-advance-healthcare-research-childrens-hospital-use-case-hadoopsummit/">silicon angle story</link>
                (Published June 2013)
                <sbr/>
            </para>
        </section>
    </section>


    <section>
        <title>Human Sciences</title>

        <section>
            <title>NextBio</title>
            <para>
                NextBio is using Hadoop MapReduce and HBase to process massive amounts of human genome data.
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis><sbr/>
                Processing multi-terabyte data sets wasn't feasible using traditional databases like MySQL.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis><sbr/>
                NextBio uses Hadoop map reduce to process genome data in batches and it uses HBase as a scalable data store
            </para>
            <para>
                <emphasis role="bold">Hadoop vendor: </emphasis><sbr/>
                Intel
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.nextbio.com/">NextBio</link><sbr/>
                <link xlink:href="http://hadoop.intel.com/pdfs/IntelNextBioCaseStudy.pdf">Intel case study</link> (<link xlink:href="cached_reports/IntelNextBioCaseStudy.pdf">cached copy</link>)
                (Published Feb 2013)
                <sbr/>
                <link xlink:href="http://www.informationweek.com/software/information-management/hbase-hadoops-next-big-data-chapter/232901601?pgno=1">Information Week article (May 2012)</link>
                (<link xlink:href="cached_reports/www.informationweek_2012_05_08.pdf">cached copy</link>)
            </para>

        </section>

    </section> <!-- end human sciences -->


    <section>
        <title>Telecoms</title>
        <section>
            <title>China Telecom Guangdong</title>
            <para>
                <emphasis role="bold">Problem: </emphasis> Storing billions of mobile call records and providing real time access to the call records and billing information to customers.  <sbr/>
                Traditional storage/database systems couldn't scale to the loads and provide a cost effective solution
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis> HBase is used to store billions of rows of call record details.  30TB of data is added monthly
            </para>
            <para>
                <emphasis role="bold">Hadoop vendor: </emphasis> Intel
            </para>
            <para>
                <emphasis role="bold">Hadoop cluster size: </emphasis> 100+ nodes
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://gd.10086.cn/">China Telecom Quangdong</link><sbr/>
                <link xlink:href="http://hadoop.intel.com/pdfs/IntelChinaMobileCaseStudy.pdf">Intel case study</link> (<link xlink:href="cached_reports/IntelChinaMobileCaseStudy.pdf">cached copy</link>)
                (Published Feb 2013)
                <sbr/>
                <link xlink:href="http://www.slideshare.net/IntelAPAC/apac-big-data-dc-strategy-update-for-idh-launch-rk">Intel APAC presentation</link>
            </para>

        </section>

        <section>
            <title>Nokia</title>
            <para>
                Nokia collects and analyzes vast amounts of data from mobile phones
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                <sbr/>
                (1) Dealing with 100TB of structured data and 500TB+ of semi-structured data <sbr/>
                (2) 10s of PB across Nokia, 1TB / day
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                HDFS data warehouse allows storing all the semi/multi structured data and offers processing data at peta byte scale
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                Cloudera
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                <sbr/>
                (1) 500TB of data <sbr/>
                (2) 10s of PB across Nokia, 1TB / day
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                (1) <link xlink:href="http://www.cloudera.com/content/dam/cloudera/Resources/PDF/Cloudera_Nokia_Case_Study_Hadoop.pdf">Cloudera case study</link>
                (<link xlink:href="cached_reports/Cloudera_Nokia_Case_Study_Hadoop.pdf">cached copy</link>)
                (Published Apr 2012)
                <sbr/>
                (2) <link xlink:href="http://cdn.oreillystatic.com/en/assets/1/event/85/Big%20Data%20Analytics%20Platform%20at%20Nokia%20%E2%80%93%20Selecting%20the%20Right%20Tool%20for%20the%20Right%20Workload%20Presentation.pdf">strata NY 2012 presentation slides</link>
                (<link xlink:href="cached_reports/Nokia_Bigdata.pdf">cached copy</link>)
                <sbr/>
                <link xlink:href="http://strataconf.com/stratany2012/public/schedule/detail/26880">Strata NY 2012 presentation</link>
            </para>
        </section>
    </section>

    <section>
        <title>Travel</title>
        <section>
            <title>Orbitz</title>
            <para>
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                Orbitz generates tremendous amounts of log data.  The raw logs are only stored for a few days because of costly data warehousing.  Orbitz needed an effective way to store and process this data, plus they needed to improve their hotel rankings.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                A Hadoop cluster provided a very cost effective way to store vast amounts of raw logs.  Data is cleaned and analyzed and machine learning algorithms are run.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://www.slideshare.net/jseidman/windy-citydb-final-4635799">Orbitz presentation</link>
                (Published 2010)
                <sbr/>
                <link xlink:href="http://www.datanami.com/datanami/2012-04-26/six_super-scale_hadoop_deployments.html">Datanami article</link>
            </para>
        </section>
    </section>

    <section>
        <title>Energy</title>
        <section>
            <title>Seismic Data at Chevron</title>
            <para>
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                Chevron analyzes vast amounts of seismic data to find potential oil reserves.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                Hadoop offers the storage capacity and processing power to analyze this data.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                IBM Big Insights
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://almaden.ibm.com/colloquium/resources/Managing%20More%20Bits%20Than%20Barrels%20Breuning.PDF">Presentation</link>
                (<link xlink:href="cached_reports/IBM_Chevron.pdf">cached copy</link>)
                (Published June 2012)
                <sbr/>
            </para>
            <para>
            </para>
        </section>
        <section>
            <title>OPower</title>
            <para>
            OPower works with utility companies to provide engaging, relevant, and personalized content about home energy use to millions of households.
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                Collecting and analyzing massive amounts of data and deriving insights into customers' energy usage.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                Hadoop provides a single storage for all the massive data and machine learning algorithms are run on the data.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                <link xlink:href="http://cdn.oreillystatic.com/en/assets/1/event/85/Data%20Science%20with%20Hadoop%20at%20Opower%20Presentation.pdf">presentation</link>
                (<link xlink:href="cached_reports/Opower.pdf">cached copy</link>)
                (Published Oct 2012)
                <sbr/>
                <link xlink:href="http://strataconf.com/stratany2012/public/schedule/detail/25736">Strata NY 2012</link>
                <sbr/>
                <link xlink:href="http://strataconf.com/strata2013/public/schedule/detail/27158">Strata 2013</link>
                <sbr/>
                <link xlink:href="http://www.opower.com">OPower.com</link>
            </para>
        </section>
    </section>

    <section>
        <title>Logistics</title>
        <section>
            <title>Trucking data @ US Xpress</title>
            <para>
                US Xpress - one of the largest trucking companies in US - is using Hadoop to store sensor data from their trucks.  The intelligence they mine out of this, saves them $6 million / year in fuel cost alone.
            </para>
            <para>
                <emphasis role="bold">Problem: </emphasis>
                Collecting and and storing 100s of data points from thousands of trucks, plus lots of geo data.
            </para>
            <para>
                <emphasis role="bold">Solution: </emphasis>
                Hadoop allows storing enormous amount of sensor data.  Also Hadoop allows querying / joining this data with other data sets.
            </para>
            <para>
                <emphasis role="bold">Hadoop Vendor: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Cluster/Data size: </emphasis>
                ?
            </para>
            <para>
                <emphasis role="bold">Links: </emphasis><sbr/>
                
                <link xlink:href="http://www.computerweekly.com/news/2240146943/Case-Study-US-Xpress-deploys-hybrid-big-data-with-Informatica">Computer Weekly article</link> (Published May 2012)
                <sbr/>
                <link xlink:href="http://hortonworks.com/wp-content/uploads/downloads/2013/06/Hortonworks.BusinessValueofHadoop.v1.0.pdf">Hortonworks white paper on 'Business Value of Hadoop'</link>                 (<link xlink:href="cached_reports/Hortonworks.BusinessValueofHadoop.v1.0.pdf">cached copy</link>) (Published July 2013)

                <sbr/>
                <link xlink:href="http://www.usxpress.com/">USXpress.com</link>
            </para>
        </section>
    </section>

</chapter>
